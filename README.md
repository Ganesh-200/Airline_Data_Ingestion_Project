# Airline Data Ingestion Project âœˆï¸
### Automated Data Pipeline using AWS Glue, Redshift & Step Functions
![project-banner.png](https://github.com/user-attachments/assets/b83ff96c-a732-434a-85c0-ed8b6c8e29b4)
(https://raw.githubusercontent.com/Ganesh-200/Airline_Data_Ingestion_Project/main/project-banner.png)

## ğŸ“Œ Overview
This project builds a scalable data ingestion pipeline for airline-related datasets using AWS services. The pipeline automatically extracts, transforms, and loads (ETL) data into Amazon Redshift, enabling efficient querying and analysis. This data pipeline is mainly designed to respond quickly for a customer queries about their flight booking information by a customer care executive effectively with the valuable insights like arrival delay time, daparture delay time, etc.

## ğŸš€ Key Features
âœ… **AWS Glue** â€“ ETL processing and data transformation  
âœ… **Amazon Redshift** â€“ Centralized data warehousing  
âœ… **Amazon S3** â€“ Storage for raw and processed data  
âœ… **AWS Glue Crawlers** â€“ Schema inference and metadata cataloging  
âœ… **AWS Step Functions** â€“ Workflow automation for end-to-end orchestration  
âœ… **Amazon SNS** â€“ Notifications for pipeline execution updates 

## ğŸ› ï¸ Tech Stack
ğŸ”¹ **AWS Services:** Glue, Redshift, S3, Step Functions, Crawlers, SNS  
ğŸ”¹ **ETL Framework:** AWS Glue Jobs (PySpark, Python)  
ğŸ”¹ **Orchestration:** AWS Step Functions  
ğŸ”¹ **Data Storage:** Amazon S3 (Raw & Processed Data), Redshift  
ğŸ”¹ **Monitoring & Alerts:** Amazon SNS  

## Workflow
![Airline_Data_Ingestion_Project_workflow](https://github.com/user-attachments/assets/520887a7-124e-44fc-9afc-47278f48e5c7)
1. A file named airport_codes.csv is already available in Amazon S3, and the associated dimension table is also present in Amazon Redshift.
2. Whenever the client uploads raw flight data on daily basisâ€”including details like airport state, origin ID, destination ID, arrival delay, and departure delayâ€”to an S3 bucket, an S3 PutObject event triggers an Amazon EventBridge rule.
3. The EventBridge rule then triggers an AWS Step Function, which orchestrates the workflow.
4. The Step Function starts an AWS Glue Crawler to scan the newly arrived data in S3 and the dimension table in Redshift, prepares metadata, and stores it in the AWS Glue Data Catalog.
5. Once the crawler completes its execution, the Step Function triggers an AWS Glue Job to process the daily flight data using the existing dimension table in Redshift.
6. After the processing is complete, AWS Glue writes the results back to Redshift for further analysis.
7. The crawler then scans the processed table in Redshift to update the metadata in the centralized catalog database.
8. The Step Function tracks each task and sends the Glue job status to an Amazon SNS topic, notifying the user.
